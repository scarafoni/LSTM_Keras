{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Here I try to re-implement a smple lSTM as illustrated at\n",
    "http://adventuresinmachinelearning.com/keras-lstm-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout, TimeDistributed, Reshape, Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] run_opt\n",
      "ipykernel_launcher.py: error: argument run_opt: invalid int value: '/run/user/1000/jupyter/kernel-d0e03b3c-0e3d-49a9-b00a-9441403ad151.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\Andy\\Documents\\simple-examples\\data\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('run_opt', type=int, default=1, help='An integer: 1 to train, 2 to test')\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='The full path of the training data')\n",
    "args = parser.parse_args()\n",
    "if args.data_path:\n",
    "    data_path = args.data_path\n",
    "\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "\n",
    "\n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n",
    "\n",
    "num_steps = 30\n",
    "batch_size = 20\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "\n",
    "hidden_size = 500\n",
    "use_dropout=True\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "num_epochs = 50\n",
    "if args.run_opt == 1:\n",
    "#     model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "#                         validation_data=valid_data_generator.generate(),\n",
    "#                         validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])\n",
    "    model.fit_generator(train_data_generator.generate(), 2000, num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=10)\n",
    "    model.save(data_path + \"final_model.hdf5\")\n",
    "elif args.run_opt == 2:\n",
    "    model = load_model(data_path + \"\\model-40.hdf5\")\n",
    "    dummy_iters = 40\n",
    "    example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "                                                     skip_step=1)\n",
    "    print(\"Training data:\")\n",
    "    for i in range(dummy_iters):\n",
    "        dummy = next(example_training_generator.generate())\n",
    "    num_predict = 10\n",
    "    true_print_out = \"Actual words: \"\n",
    "    pred_print_out = \"Predicted words: \"\n",
    "    for i in range(num_predict):\n",
    "        data = next(example_training_generator.generate())\n",
    "        prediction = model.predict(data[0])\n",
    "        predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "        true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "        pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "    print(true_print_out)\n",
    "    print(pred_print_out)\n",
    "    # test data set\n",
    "    dummy_iters = 40\n",
    "    example_test_generator = KerasBatchGenerator(test_data, num_steps, 1, vocabulary,\n",
    "                                                     skip_step=1)\n",
    "    print(\"Test data:\")\n",
    "    for i in range(dummy_iters):\n",
    "        dummy = next(example_test_generator.generate())\n",
    "    num_predict = 10\n",
    "    true_print_out = \"Actual words: \"\n",
    "    pred_print_out = \"Predicted words: \"\n",
    "    for i in range(num_predict):\n",
    "        data = next(example_test_generator.generate())\n",
    "        prediction = model.predict(data[0])\n",
    "        predict_word = np.argmax(prediction[:, num_steps - 1, :])\n",
    "        true_print_out += reversed_dictionary[test_data[num_steps + dummy_iters + i]] + \" \"\n",
    "        pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "    print(true_print_out)\n",
    "    print(pred_print_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
